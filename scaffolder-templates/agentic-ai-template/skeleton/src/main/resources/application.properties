# Default LangChain4J configuration
quarkus.langchain4j.parasol-chat.chat-model.provider=openai
quarkus.langchain4j.log-requests=true
quarkus.langchain4j.log-responses=true
quarkus.langchain4j.parasol-chat.chat-model.stop=DONE,done,stop,STOP

# OpenAI
quarkus.langchain4j.openai.parasol-chat.chat-model.temperature=0.3
quarkus.langchain4j.openai.parasol-chat.timeout=600s
quarkus.langchain4j.openai.parasol-chat.chat-model.model-name=parasol-chat
quarkus.langchain4j.openai.parasol-chat.base-url=http://parasol-chat-predictor.aiworkshop.svc.cluster.local:8080/v1
# quarkus.tls.trust-all=true

# Ollama
quarkus.langchain4j.ollama.parasol-chat.timeout=600s
quarkus.langchain4j.ollama.parasol-chat.chat-model.model-id=llama3.2
quarkus.langchain4j.ollama.parasol-chat.chat-model.temperature=0.3
%ollama.quarkus.langchain4j.parasol-chat.chat-model.provider=ollama

# Jlama
# Uncomment to use the un-quantized model
# NOTE: This doesn't work on MacOS nor on GitHub action CI (due to the size of the model)
#quarkus.langchain4j.jlama.parasol-chat.chat-model.model-name=rh-rad-ai-roadshow/parasol-trained-chat
# Use the quantized model by default
quarkus.langchain4j.jlama.parasol-chat.chat-model.model-name=mariofusco/parasol-trained-chat-JQ4
quarkus.langchain4j.jlama.parasol-chat.chat-model.temperature=0.3
%jlama.quarkus.langchain4j.parasol-chat.chat-model.provider=jlama

# Http
quarkus.http.host=0.0.0.0
quarkus.http.port=${{values.port}}
quarkus.http.cors=true
quarkus.http.cors.origins=*
quarkus.dev-ui.cors.enabled=false
quarkus.dev-ui.hosts=${{values.owner}}-parasol-insurance-quarkus-devui${{values.cluster}}

# Hibernate
quarkus.hibernate-orm.physical-naming-strategy=org.hibernate.boot.model.naming.CamelCaseToUnderscoresNamingStrategy

# Logging
%dev,test.quarkus.log.category."org.parasol".level=DEBUG
%dev,test.quarkus.log.console.level=DEBUG

# Quinoa
quarkus.quinoa.package-manager-install=true
quarkus.quinoa.package-manager-install.node-version=22.2.0
quarkus.quinoa.package-manager-install.npm-version=10.8.1
quarkus.quinoa.build-dir=dist
quarkus.quinoa.enable-spa-routing=true

# H2
%prod.quarkus.datasource.jdbc.url=jdbc:h2:mem:claims;DB_CLOSE_DELAY=-1
%prod.quarkus.datasource.username=sa
%prod.quarkus.datasource.password=sa
%prod.quarkus.hibernate-orm.sql-load-script=import.sql
%prod.quarkus.hibernate-orm.database.generation=drop-and-create

# Websockets
quarkus.websockets-next.server.auto-ping-interval=1m

# OpenTelemetry & Tracing
quarkus.otel.enabled=true
quarkus.datasource.jdbc.telemetry=${quarkus.otel.enabled}
quarkus.otel.exporter.otlp.traces.endpoint=http://otel-collector.parasol-app-${{values.owner}}-dev.svc.cluster.local:4317


{% if values.ai_model == 'granite-3.1' %}
quarkus.langchain4j.openai.agentic-ai.chat-model.temperature=0.3
quarkus.langchain4j.openai.agentic-ai.timeout=600s
quarkus.langchain4j.openai.agentic-ai.chat-model.model-name=granite-3.1
quarkus.langchain4j.openai.agentic-ai.base-url=http://granite3-predictor.aiworkshop.svc.cluster.local:8080/v1
{% endif %}

{% if values.ai_model == 'llama3.3' %}
quarkus.langchain4j.openai.agentic-ai.chat-model.temperature=0.3
quarkus.langchain4j.openai.agentic-ai.timeout=600s
quarkus.langchain4j.openai.agentic-ai.chat-model.model-name=llama3.3
quarkus.langchain4j.openai.agentic-ai.base-url=http://llama3-predictor.aiworkshop.svc.cluster.local:8080/v1
{% endif %}

{% if 'BraveSearch' in values.mcp_server %}
quarkus.langchain4j.mcp.bravesearch.transport-type=stdio
quarkus.langchain4j.mcp.bravesearch.command=npm,exec,@modelcontextprotocol/server-brave-search@0.6.2
quarkus.langchain4j.mcp.bravesearch.environment.BRAVE_API_KEY=demo_brave_key
{% endif %}

{% if 'GoogleMaps' in values.mcp_server %}
quarkus.langchain4j.mcp.googlemaps.transport-type=stdio
quarkus.langchain4j.mcp.googlemaps.command=npm,exec,@modelcontextprotocol/server-google-maps@0.6.2
quarkus.langchain4j.mcp.googlemaps.environment.GOOGLE_MAPS_API_KEY=demo_gmaps_key
{% endif %}

{% if 'Memory' in values.mcp_server %}
quarkus.langchain4j.mcp.memory.transport-type=stdio
quarkus.langchain4j.mcp.memory.command=npm,exec,@modelcontextprotocol/server-memory@0.6.2
{% endif %}
